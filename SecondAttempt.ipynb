{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import time\n",
    "\n",
    "batch_size = 32\n",
    "nb_categories = 101\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/wenjian/data4/'\n",
    "\n",
    "train_set_dir = data_root + 'wiki_crop_train/'\n",
    "val_set_dir = data_root + 'wiki_crop_val/'\n",
    "test_set_dir = data_root + 'wiki_crop_test/'\n",
    "\n",
    "result_dir = '/home/wenjian/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])]) # Imagenet standards\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                [0.229, 0.224, 0.225])]) # Imagenet standards\n",
    "\n",
    "# What should we do for test??????\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "train_set = ImageFolder(train_set_dir, transform=train_transform, target_transform=None)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_set = ImageFolder(val_set_dir, transform=val_transform, target_transform=None)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "test_set = ImageFolder(test_set_dir, transform=test_transform, target_transform=None)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGGmodel = models.vgg16(pretrained=True)\n",
    "for param in VGGmodel.parameters():\n",
    "    param.requires_grad = False\n",
    "# The original classifier[6]'s input features\n",
    "n_inputs = VGGmodel.classifier[3].out_features\n",
    "# Add on classifier\n",
    "VGGmodel.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(n_inputs, 512), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Dropout(0.4),\n",
    "                      nn.Linear(512, nb_categories),                   \n",
    "                      nn.Softmax())\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input size of the image 256 256 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input_tensor):\n",
    "        return input_tensor.view(input_tensor.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(9*1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 101),\n",
    "            nn.Softmax())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('---0---', x.shape)\n",
    "        x = self.layer1(x)\n",
    "        #print('---1---', x.shape)\n",
    "        x = self.layer2(x)\n",
    "        #print('---2---', x.shape)\n",
    "        x = self.layer3(x)\n",
    "        #print('---3---', x.shape)\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGGmodel\n",
    "criterion = nn.KLDivLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_shadow(label_batch, gap=0.2, nb_categories=101):\n",
    "    batch_size = label_batch.shape[0]\n",
    "    l = torch.zeros(batch_size, nb_categories, dtype= torch.float)\n",
    "    for i in range(batch_size):\n",
    "        label = torch.tensor(label_batch[i], dtype=torch.float)\n",
    "        for j in range(nb_categories):\n",
    "            l[i,j] = 1- abs(label-j)*gap\n",
    "            if l[i,j]<0:\n",
    "                l[i,j]=0\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(vectors):\n",
    "    # To normalize the distribution. Shape of the vectors are expected as (batch_size, vector_dim)\n",
    "    # Noted that the softmax function in Pytorch only work for float type tensor\n",
    "    return nn.functional.softmax(vectors, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    print('Start training...')\n",
    "    model.train()\n",
    "    log_interval = 10\n",
    "    losses = []\n",
    "    with open(result_dir + 'training_log.txt', 'a') as f: \n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            log_proba = torch.log(out)\n",
    "            labels = linear_shadow(labels)\n",
    "            labels = target_distribution(labels) # Normalize the distribution\n",
    "            loss = criterion(log_proba, labels)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "                f.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader):\n",
    "    print(\"Start validation...\")\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    diff = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            out = model(data)\n",
    "            log_proba = torch.log(out)\n",
    "            labels = linear_shadow(target)\n",
    "            labels = target_distribution(labels) # Normalize th distribution\n",
    "            test_loss += criterion(log_proba, labels)\n",
    "            preds = out.argmax(dim=1, keepdim=True)\n",
    "            diff += nn.functional.l1_loss(preds.float(), target.float()) #nn.functional.l1_loss output the average l1 distance by default\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    diff /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, diff: {}\\n'.format(test_loss, diff))\n",
    "    with open(result_dir + 'validation_log.txt', 'a') as f:\n",
    "        f.write('\\nTest set: Average loss: {:.4f}, diff: {}\\n'.format(test_loss, diff))\n",
    "    return test_loss, diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th epoch started...\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenjian/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "/home/wenjian/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1906: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4138 (0%)]\tLoss: 0.000490\n",
      "Train Epoch: 0 [320/4138 (8%)]\tLoss: 0.000249\n",
      "Train Epoch: 0 [640/4138 (15%)]\tLoss: 0.000248\n",
      "Train Epoch: 0 [960/4138 (23%)]\tLoss: 0.000245\n",
      "Train Epoch: 0 [2560/4138 (62%)]\tLoss: 0.000242\n",
      "Train Epoch: 0 [2880/4138 (69%)]\tLoss: 0.000238\n",
      "Train Epoch: 0 [3200/4138 (77%)]\tLoss: 0.000247\n",
      "Train Epoch: 0 [3520/4138 (85%)]\tLoss: 0.000240\n",
      "Train Epoch: 0 [3840/4138 (92%)]\tLoss: 0.000239\n",
      "Start validation...\n",
      "\n",
      "Test set: Average loss: 0.0000, diff: 0.8021881580352783\n",
      "\n",
      "0 th epoch time:  513.2470374107361\n",
      "1 th epoch started...\n",
      "Start training...\n",
      "Train Epoch: 1 [0/4138 (0%)]\tLoss: 0.000239\n",
      "Train Epoch: 1 [320/4138 (8%)]\tLoss: 0.000238\n",
      "Train Epoch: 1 [640/4138 (15%)]\tLoss: 0.000234\n",
      "Train Epoch: 1 [960/4138 (23%)]\tLoss: 0.000240\n",
      "Train Epoch: 1 [1280/4138 (31%)]\tLoss: 0.000238\n",
      "Train Epoch: 1 [1600/4138 (38%)]\tLoss: 0.000242\n",
      "Train Epoch: 1 [1920/4138 (46%)]\tLoss: 0.000236\n",
      "Train Epoch: 1 [2240/4138 (54%)]\tLoss: 0.000245\n",
      "Train Epoch: 1 [2560/4138 (62%)]\tLoss: 0.000244\n",
      "Train Epoch: 1 [2880/4138 (69%)]\tLoss: 0.000238\n",
      "Train Epoch: 1 [3200/4138 (77%)]\tLoss: 0.000236\n",
      "Train Epoch: 1 [3520/4138 (85%)]\tLoss: 0.000240\n",
      "Train Epoch: 1 [3840/4138 (92%)]\tLoss: 0.000236\n",
      "Start validation...\n",
      "\n",
      "Test set: Average loss: 0.0000, diff: 0.6616062521934509\n",
      "\n",
      "1 th epoch time:  507.07794880867004\n",
      "2 th epoch started...\n",
      "Start training...\n",
      "Train Epoch: 2 [0/4138 (0%)]\tLoss: 0.000243\n",
      "Train Epoch: 2 [320/4138 (8%)]\tLoss: 0.000237\n",
      "Train Epoch: 2 [640/4138 (15%)]\tLoss: 0.000231\n",
      "Train Epoch: 2 [960/4138 (23%)]\tLoss: 0.000233\n",
      "Train Epoch: 2 [1280/4138 (31%)]\tLoss: 0.000240\n",
      "Train Epoch: 2 [1600/4138 (38%)]\tLoss: 0.000235\n",
      "Train Epoch: 2 [1920/4138 (46%)]\tLoss: 0.000242\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "train_losses_all = []\n",
    "val_loss_all = []\n",
    "val_diff_all = []\n",
    "for epoch in range(n_epochs):\n",
    "    print(epoch,'th epoch started...')\n",
    "    epoch_start = time.time()\n",
    "    train_losses = train(model, train_loader, criterion, optimizer, epoch)\n",
    "    train_losses_all.append(train_losses)\n",
    "    val_loss, diff = validate(model, val_loader)\n",
    "    val_loss_all.append(val_loss)\n",
    "    val_diff_all.append(diff)\n",
    "    torch.save(model.state_dict(),\"model_trained_\" + str(epoch) + \".pt\")\n",
    "\n",
    "    train_loss_array = np.array(train_losses_all)\n",
    "    val_loss_array = np.array(val_loss_all)\n",
    "    val_diff_array = np.array(val_diff_all)\n",
    "\n",
    "    np.save(result_dir + 'train_loss_array.npy', train_loss_array)\n",
    "    np.save(result_dir + 'val_loss_array.npy', val_loss_array)\n",
    "    np.save(result_dir + 'val_diff_array.npy', val_diff_array)\n",
    "    \n",
    "    epoch_end = time.time()\n",
    "    epoch_time = epoch_end - epoch_start\n",
    "    print(epoch, 'th epoch time: ', epoch_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img_name):\n",
    "    img = Image.open(img_name)\n",
    "    img = torchvision.transforms.functional.to_tensor(img)\n",
    "    img = torchvision.transforms.functional.to_pil_image(img)\n",
    "    img = torchvision.transforms.functional.resize(img, 256)\n",
    "    five_imgs = torchvision.transforms.functional.five_crop(img, size=224)\n",
    "    img = five_imgs[4] # Center crop\n",
    "    img = torchvision.transforms.functional.to_tensor(img)\n",
    "    #print('---aaa---', img.shape)\n",
    "    batch_img = img.view(1, *(img.shape))\n",
    "    #print('---bbb---', batch_img.shape)\n",
    "    with torch.no_grad():\n",
    "        out = model(batch_img)\n",
    "        print(out)\n",
    "        pred = out.argmax()\n",
    "        print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "img_name = '/home/wenjian/data3/wiki_crop_test/48/10648596_1962-08-25_2010.jpg'\n",
    "IPyImage(filename=img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0095, 0.0096, 0.0098, 0.0099, 0.0099, 0.0099, 0.0099, 0.0098, 0.0096,\n",
      "         0.0096, 0.0095, 0.0095, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0097,\n",
      "         0.0096, 0.0097, 0.0096, 0.0097, 0.0096, 0.0096, 0.0096, 0.0096, 0.0097,\n",
      "         0.0097, 0.0097, 0.0097, 0.0097, 0.0098, 0.0097, 0.0097, 0.0096, 0.0095,\n",
      "         0.0095, 0.0095, 0.0096, 0.0095, 0.0096, 0.0096, 0.0096, 0.0096, 0.0097,\n",
      "         0.0098, 0.0098, 0.0098, 0.0097, 0.0097, 0.0097, 0.0098, 0.0097, 0.0098,\n",
      "         0.0098, 0.0097, 0.0098, 0.0097, 0.0098, 0.0099, 0.0100, 0.0101, 0.0100,\n",
      "         0.0102, 0.0101, 0.0101, 0.0101, 0.0102, 0.0100, 0.0101, 0.0101, 0.0102,\n",
      "         0.0103, 0.0103, 0.0103, 0.0105, 0.0105, 0.0105, 0.0103, 0.0104, 0.0104,\n",
      "         0.0104, 0.0105, 0.0106, 0.0108, 0.0107, 0.0107, 0.0106, 0.0106, 0.0105,\n",
      "         0.0103, 0.0103, 0.0102, 0.0102, 0.0101, 0.0100, 0.0098, 0.0097, 0.0096,\n",
      "         0.0095, 0.0094]])\n",
      "tensor(84)\n"
     ]
    }
   ],
   "source": [
    "predict(model, img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([1.1, 2.5, 3.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.l1_loss(a.float() ,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
